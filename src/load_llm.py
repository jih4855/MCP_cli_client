import requests
from src.memory import MemoryManager
import google.generativeai as genai
from openai import OpenAI

class LLM:
    def __init__(self, model, system_prompt, provider, api_key=None, api_base=None,session_id=None, max_context=20, mcp_manager=None):
        self.model = model
        self.system_prompt = system_prompt
        self.memory = MemoryManager()
        self.provider = provider
        self.api_key = api_key
        self.api_base = api_base
        self.session_id = session_id
        self.max_context = max_context
        self.mcp_manager = mcp_manager  # â† ì¶”ê°€!

    async def check_provider(self, user_input, tools=None):
        if self.provider.lower() == "ollama":
            return await self.chat_ollama(user_input, tools)
        elif self.provider.lower() == "gemini":
            return await self.chat_gemini(user_input, tools)
        elif self.provider.lower() == "openai":
            return await self.chat_openai(user_input, tools)
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì œê³µì: {self.provider}")

    async def chat_ollama(self, user_input, tools=None):
        """
        user_input: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸
        tools: MCPì—ì„œ ê°€ì ¸ì˜¨ ë„êµ¬ ëª©ë¡
        max_context: ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ìµœëŒ€ ëŒ€í™” ê°œìˆ˜
        """
        self.user_input = user_input
        # ìµœê·¼ max_contextê°œ ëŒ€í™”ë§Œ ê°€ì ¸ì˜¤ê¸°
        history = self.memory.get_history(self.session_id)

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        system_content = self.system_prompt
        
        # ìµœê·¼ ëŒ€í™”ë§Œ ì¶”ê°€ (user/assistant ìŒìœ¼ë¡œ ê³„ì‚°)
        if len(history) > self.max_context:
            history = history[-self.max_context:]

        # tools ì •ê·œí™”
        normalized_tools = _normalize_tools(tools)
        ollama_tools = []
        for tool in normalized_tools:
            ollama_tools.append({
                "name": tool["name"],
                "description": tool.get("description", ""),
                "inputSchema": tool.get("inputSchema", {})
            })

        try:
            print(f"Ollama ëª¨ë¸ ì‚¬ìš©: {self.model}")
            print(f"ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ")
           
            messages = [{"role": "system", "content": system_content}]
        # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            if history:
                for message in history:
                    messages.append({
                        "role": message["role"],
                        "content": message["content"]
                    })

            # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
            messages.append({"role": "user", "content": self.user_input})

            # ë””ë²„ê¹… ì •ë³´
            print(f"ğŸ” ë””ë²„ê·¸ - ë©”ì‹œì§€ ê°œìˆ˜: {len(messages)}")
        

            # Ollama API í˜¸ì¶œ
            payload = {
                "model": self.model,
                "messages": messages,  # â† prompt ëŒ€ì‹  messages
                "tools": ollama_tools,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "num_predict": 4000
                }
            }

            print(f"ğŸ” ë””ë²„ê·¸ - í˜ì´ë¡œë“œ: {payload}")
            response = requests.post(
                self.api_base,
                json=payload,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result["message"]["content"].strip()
        
                # ëŒ€í™” ê¸°ë¡ì— ìƒˆ ë©”ì‹œì§€ë“¤ ì¶”ê°€
                history.append({"role": "user", "content": self.user_input})
                history.append({"role": "assistant", "content": ai_response})
                self.memory.save_history(self.session_id, history)

                return ai_response
            else:
                print(f"Ollama API ì˜¤ë¥˜: {response.status_code}")
                print(f"ì‘ë‹µ: {response.text}")
                return None
                
        except Exception as e:
            print(f"ëª¨ë¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None

    async def chat_gemini(self, user_input, tools=None):
        # ìµœê·¼ max_contextê°œ ëŒ€í™”ë§Œ ê°€ì ¸ì˜¤ê¸°
        history = self.memory.get_history(self.session_id)
        self.user_input = user_input
        # ìµœê·¼ ëŒ€í™”ë§Œ ì¶”ê°€ (user/assistant ìŒìœ¼ë¡œ ê³„ì‚°)
        if len(history) > self.max_context:
            history = history[-self.max_context:]

        # tools ì •ê·œí™”
        normalized_tools = _normalize_tools(tools)

        try: 
            print(f"Gemini ëª¨ë¸ ì‚¬ìš©: {self.model}")
            print(f"ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ")
            
            # API í‚¤ ì„¤ì •
            genai.configure(api_key=self.api_key)
            
            # ëª¨ë¸ ê°ì²´ ìƒì„±
            model = genai.GenerativeModel(self.model)
            
            # ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = self.system_prompt + "\n\n"
            
            # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            if history:
                for message in history:
                    role = "ì‚¬ìš©ì" if message["role"] == "user" else "AI"
                    full_prompt += f"{role}: {message['content']}\n"
            
            # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
            full_prompt += f"ì‚¬ìš©ì: {self.user_input}\nAI: "
            
            # toolsê°€ ìˆë‹¤ë©´ function calling ì„¤ì •
            # print(f"ğŸ” tools ë§¤ê°œë³€ìˆ˜: {tools}")
            # print(f"ğŸ” normalized_tools: {normalized_tools}")
            # print(f"ğŸ” normalized_tools ê¸¸ì´: {len(normalized_tools) if normalized_tools else 0}")
            if normalized_tools:
                gemini_tools = []
                for tool in normalized_tools:
                    input_schema = tool.get("inputSchema", {})
                    print(f"ğŸ” ë„êµ¬ '{tool['name']}' inputSchema: {input_schema}")
                    # Gemini function calling í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    gemini_tools.append(
                        genai.protos.Tool(
                            function_declarations=[
                                genai.protos.FunctionDeclaration(
                                    name=tool["name"],
                                    description=tool.get("description", ""),
                                    parameters=genai.protos.Schema(
                                        type=genai.protos.Type.OBJECT,
                                        properties=_convert_to_protos_properties(input_schema),
                                        required=input_schema.get("required", [])
                                    )
                                )
                            ]
                        )
                    )
                # print(f"ë„êµ¬ í¬í•¨: {len(gemini_tools)}ê°œ")

                # toolsì™€ í•¨ê»˜ ìƒì„±
                response = model.generate_content(
                    full_prompt,
                    tools=gemini_tools
                )
                
                function_calls_part = []
                for part in response.candidates[0].content.parts:
                    if hasattr(part, 'function_call') and part.function_call:
                        function_calls_part.append(part.function_call)  # function_call ê°ì²´ë§Œ ì¶”ê°€

                if function_calls_part:
                    try:
                        all_results = []
                        for tool_calling in function_calls_part:
                            tool_result = await self.mcp_manager.multi_execute_tool(
                                tool_calling.name,  # ì´ì œ ì˜¬ë°”ë¥¸ ì ‘ê·¼
                                _convert_proto_args_to_dict(tool_calling.args)
                            )
                            all_results.append(f"ë„êµ¬ '{tool_calling.name}': {tool_result}")
                            print(f"âœ… ë„êµ¬ '{tool_calling.name}' ì‹¤í–‰ ê²°ê³¼: {tool_result}")

                        # ëª¨ë“  ê²°ê³¼ë¥¼ í¬í•¨í•œ ìµœì¢… ì‘ë‹µ
                        results_text = "\n".join(all_results)
                        final_prompt = f"{full_prompt}\në„êµ¬ ì‹¤í–‰ ê²°ê³¼:\n{results_text}\nì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."
                        final_response = model.generate_content(final_prompt)
                        ai_response_text = final_response.candidates[0].content.parts[0].text.strip()

                    except Exception as e:
                        print(f"âŒ ë„êµ¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                        ai_response_text = f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                else:
                  # ë„êµ¬ í˜¸ì¶œ ì—†ì„ ë•Œ
                  ai_response_text = response.candidates[0].content.parts[0].text.strip()

            else:
                # tools ì—†ì´ ìƒì„±
                response = model.generate_content(full_prompt)
                print(f"ğŸ” response êµ¬ì¡°: {type(response)}")
                print(f"ğŸ” candidates: {len(response.candidates) if response.candidates else 0}")
                ai_response_text = response.candidates[0].content.parts[0].text.strip()
                print("ë„êµ¬ ì—†ìŒ")
            
            # ëŒ€í™” ê¸°ë¡ì— ìƒˆ ë©”ì‹œì§€ë“¤ ì¶”ê°€
            history.append({"role": "user", "content": self.user_input})
            history.append({"role": "assistant", "content": ai_response_text})
            self.memory.save_history(self.session_id, history)

            return ai_response_text
    
        except Exception as e:
            print(f"ëª¨ë¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None

    def chat_openai(self, user_input, tools=None):
            """
            user_input: ì‚¬ìš©ìê°€ ì…ë ¥í•œ í…ìŠ¤íŠ¸
            tools: MCPì—ì„œ ê°€ì ¸ì˜¨ ë„êµ¬ ëª©ë¡
            max_context: ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ìµœëŒ€ ëŒ€í™” ê°œìˆ˜
            """
            self.user_input = user_input        
            # ìµœê·¼ max_contextê°œ ëŒ€í™”ë§Œ ê°€ì ¸ì˜¤ê¸°
            history = self.memory.get_history(self.session_id)
            
            # ìµœê·¼ ëŒ€í™”ë§Œ ì¶”ê°€ (user/assistant ìŒìœ¼ë¡œ ê³„ì‚°)
            if len(history) > self.max_context:
                history = history[-self.max_context:]
            
            # tools ì •ê·œí™”
            normalized_tools = _normalize_tools(tools)
            OpenAI_tools = []
            for tool in normalized_tools:
                OpenAI_tools.append({
                    "type": "function",
                    "function": {
                        "name": tool["name"],
                        "description": tool.get("description", ""),
                        "parameters": tool.get("inputSchema", {})
                    }
                })

            try: 
                print(f"OpenAI ëª¨ë¸ ì‚¬ìš©: {self.model}")
                print(f"ëŒ€í™” ê¸°ë¡: {len(history)}ê°œ")
                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì…ë ¥ ê²°í•©
                messages = [{"role": "system", "content": self.system_prompt}]
                # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
                if history:
                    for message in history:
                        messages.append({
                            "role": message["role"],
                            "content": message["content"]
                        })
            
                client = OpenAI(api_key=self.api_key)

                # messages ë¦¬ìŠ¤íŠ¸ êµ¬ì„± (Ollamaì™€ ìœ ì‚¬í•œ ë°©ì‹)
                messages = [
                    {"role": "system", "content": self.system_prompt},
                    {"role": "user", "content": self.user_input}
                ]
                if tools:
                    print(f"ë„êµ¬ í¬í•¨: {len(OpenAI_tools)}ê°œ")
                    response = client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        tools=OpenAI_tools if tools else [],
                    )
                else:
                    print("ë„êµ¬ ì—†ìŒ")
                    response = client.chat.completions.create(
                        model=self.model,
                        messages=messages
                    )

                ai_response = response.choices[0].message.content.strip()
                # ëŒ€í™” ê¸°ë¡ì— ìƒˆ ë©”ì‹œì§€ë“¤ ì¶”ê°€
                history.append({"role": "user", "content": self.user_input})
                history.append({"role": "assistant", "content": ai_response})
                self.memory.save_history(self.session_id, history)

                return ai_response
            
            except Exception as e:
                print(f"ëª¨ë¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                return None




def _convert_to_protos_properties(input_schema):
    """JSON ìŠ¤í‚¤ë§ˆë¥¼ genai.protos.Schema propertiesë¡œ ë³€í™˜"""
    if not input_schema or not isinstance(input_schema, dict):
        return {}

    properties = {}
    schema_props = input_schema.get("properties", {})

    for prop_name, prop_info in schema_props.items():
        prop_type = prop_info.get("type", "string")

        # íƒ€ì… ë§¤í•‘
        if prop_type == "string":
            proto_type = genai.protos.Type.STRING
            schema_obj = genai.protos.Schema(
                type=proto_type,
                description=prop_info.get("description", "")
            )
        elif prop_type == "integer":
            proto_type = genai.protos.Type.INTEGER
            schema_obj = genai.protos.Schema(
                type=proto_type,
                description=prop_info.get("description", "")
            )
        elif prop_type == "number":
            proto_type = genai.protos.Type.NUMBER
            schema_obj = genai.protos.Schema(
                type=proto_type,
                description=prop_info.get("description", "")
            )
        elif prop_type == "boolean":
            proto_type = genai.protos.Type.BOOLEAN
            schema_obj = genai.protos.Schema(
                type=proto_type,
                description=prop_info.get("description", "")
            )
        elif prop_type == "array":
            proto_type = genai.protos.Type.ARRAY
            # arrayì˜ items ì²˜ë¦¬
            items_info = prop_info.get("items", {"type": "string"})
            items_type = items_info.get("type", "string")

            if items_type == "string":
                items_proto_type = genai.protos.Type.STRING
            elif items_type == "integer":
                items_proto_type = genai.protos.Type.INTEGER
            elif items_type == "number":
                items_proto_type = genai.protos.Type.NUMBER
            else:
                items_proto_type = genai.protos.Type.STRING

            schema_obj = genai.protos.Schema(
                type=proto_type,
                description=prop_info.get("description", ""),
                items=genai.protos.Schema(type=items_proto_type)
            )
        else:
            proto_type = genai.protos.Type.STRING  # ê¸°ë³¸ê°’
            schema_obj = genai.protos.Schema(
                type=proto_type,
                description=prop_info.get("description", "")
            )

        properties[prop_name] = schema_obj

    return properties
  
def _convert_proto_args_to_dict(proto_args):
    """Protocol Buffers argsë¥¼ Python dictë¡œ ë³€í™˜ (ê°œì„ ëœ ë²„ì „)"""
    result = {}

    # MapComposite íƒ€ì… ì²´í¬
    if hasattr(proto_args, 'items'):
        # MapCompositeì¸ ê²½ìš°
        for key, value in proto_args.items():
            result[key] = _extract_proto_value(value)
        return result

    # RepeatedCompositeì¸ ê²½ìš° (ê¸°ì¡´ ë¡œì§)
    if hasattr(proto_args, 'fields'):
        for field in proto_args.fields:
            key = field.key
            value_obj = field.value
            result[key] = _extract_proto_value(value_obj)
        return result

    # ì§ì ‘ dict ë³€í™˜ ì‹œë„
    try:
        return dict(proto_args)
    except:
        return {}

def _extract_proto_value(value_obj):
      """Protocol Buffers ê°’ ì¶”ì¶œ"""
      if hasattr(value_obj, 'string_value') and value_obj.HasField('string_value'):
          return value_obj.string_value
      elif hasattr(value_obj, 'number_value') and value_obj.HasField('number_value'):
          return value_obj.number_value
      elif hasattr(value_obj, 'bool_value') and value_obj.HasField('bool_value'):
          return value_obj.bool_value
      elif hasattr(value_obj, 'list_value') and value_obj.HasField('list_value'):
          # ë°°ì—´ ì²˜ë¦¬
          result = []
          for item in value_obj.list_value.values:
              result.append(_extract_proto_value(item))
          return result
      else:
          # ê¸°ë³¸ì ìœ¼ë¡œ ë¬¸ìì—´ ë³€í™˜ ì‹œë„
          return str(value_obj)

def _unwrap_tool(item):
    """íŠœí”Œì´ë©´ ë‘ ë²ˆì§¸ ìš”ì†Œ(tool)ë¥¼ ë°˜í™˜, ì•„ë‹ˆë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜"""
    if isinstance(item, (tuple, list)) and len(item) == 2:
        return item[1]
    return item

def _get_tool_attr(tool, attr, default=None):
    """toolì—ì„œ ì†ì„±ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    if isinstance(tool, dict):
        return tool.get(attr, default)
    return getattr(tool, attr, default)

def _normalize_tools(tools):
        if not tools:
            return []
        
        actual_tools = []
        for item in tools:
            if isinstance(item, (tuple, list)) and len(item) == 2 and item[0] == "tools":
                actual_tools.extend(item[1])
            else:
                actual_tools.append(item)

        normalized = []
        for tool in actual_tools:  # â† ê° ë„êµ¬ë§ˆë‹¤ ì²˜ë¦¬!
            name = _get_tool_attr(tool, "name")
            if name:
                description = _get_tool_attr(tool, "description", "")
                input_schema = (
                    _get_tool_attr(tool, "inputSchema", {}) or
                    _get_tool_attr(tool, "parameters", {}) or
                    _get_tool_attr(tool, "schema", {}) or
                    {}
                )

                normalized.append({
                    "name": name,
                    "description": description,
                    "inputSchema": input_schema
                })

        return normalized